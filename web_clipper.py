import os
import time
import asyncio
import concurrent.futures
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import aiohttp
import requests
from github import Github
import openai
import telegram
from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Header, Request, Body
import uvicorn
from concurrent.futures import ThreadPoolExecutor
import shutil
from fastapi.security import APIKeyHeader, HTTPBearer, HTTPAuthorizationCredentials
from typing import Optional
import secrets
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import re
from bs4 import BeautifulSoup
from fastapi.responses import JSONResponse
import html2text
from fastapi.concurrency import run_in_threadpool
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("web_clipper.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("web_clipper")

# ä»é…ç½®æ–‡ä»¶å¯¼å…¥é…ç½®
try:
    from config import CONFIG
except ImportError:
    CONFIG = {
        'github_token': os.getenv('GITHUB_TOKEN'),
        'telegram_token': os.getenv('TELEGRAM_TOKEN'),
        'telegram_chat_id': os.getenv('TELEGRAM_CHAT_ID'),
        'openai_api_key': os.getenv('OPENAI_API_KEY'),
        'openai_base_url': os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1'),
        'github_repo': os.getenv('GITHUB_REPO'),
        'github_pages_domain': os.getenv('GITHUB_PAGES_DOMAIN', 'github.io'),
        'api_key': os.getenv('API_KEY', secrets.token_urlsafe(32)),
        'max_file_size': 10 * 1024 * 1024,
        'allowed_extensions': ['.html', '.htm'],
        'max_concurrent_requests': 50,
        'max_workers': 20,
        'request_timeout': 300,
        'max_retries': 3,
        'retry_delay': 2,
        'github_pages_max_retries': 60
    }

# é…ç½®é™åˆ¶
MAX_FILE_SIZE = CONFIG.get('max_file_size', 10 * 1024 * 1024)
ALLOWED_EXTENSIONS = set(CONFIG.get('allowed_extensions', ['.html', '.htm']))

# åˆ›å»ºåº”ç”¨å’Œé™é€Ÿå™¨
app = FastAPI(title="Web Clipper API", version="1.0.0")
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# å…¨å±€å˜é‡
handler = None
UPLOAD_DIR = Path("uploads")
executor = ThreadPoolExecutor(max_workers=CONFIG.get('max_workers', 20))

# å®‰å…¨éªŒè¯
async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(HTTPBearer())):
    if credentials.credentials != CONFIG['api_key']:
        raise HTTPException(
            status_code=401,
            detail="Invalid API key",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return credentials.credentials

class WebClipperHandler:
    def __init__(self, config):
        self.config = config
        self.github_token = config['github_token']
        self.github_repo_name = config['github_repo']
        self.telegram_bot = telegram.Bot(token=config['telegram_token'])
        
        # åˆ›å»ºå¼‚æ­¥ OpenAI å®¢æˆ·ç«¯
        self.openai_client = openai.AsyncOpenAI(
            api_key=config['openai_api_key'],
            base_url=config.get('openai_base_url')
        )
        
        # åˆ›å»º aiohttp ä¼šè¯
        self.session = None

    async def init_session(self):
        """åˆå§‹åŒ– aiohttp ä¼šè¯"""
        if self.session is None:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=self.config.get('request_timeout', 300))
            )

    async def close_session(self):
        """å…³é—­ aiohttp ä¼šè¯"""
        if self.session:
            await self.session.close()
            self.session = None

    async def process_file(self, file_path: Path, original_url: str = ''):
        """å¼‚æ­¥å¤„ç†æ–‡ä»¶"""
        try:
            logger.info("Start new web clipper processing...")
            
            # åˆå§‹åŒ–ä¼šè¯
            await self.init_session()

            # 1. ä¸Šä¼ åˆ° GitHub Pagesï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé˜»å¡æ“ä½œï¼‰
            filename, github_url = await run_in_threadpool(
                self.upload_to_github, str(file_path)
            )
            logger.info(f"Github upload success: {github_url}")

            # 2. è·å– markdown å†…å®¹
            md_content = await self.url2md_async(github_url)
            
            # 3. è·å–æ ‡é¢˜
            title = await run_in_threadpool(self.get_page_content_by_md, md_content)
            logger.info(f"Page title: {title}")
            
            # å¦‚æœæ²¡æœ‰æä¾›åŸå§‹ URLï¼Œåˆ™ä»æ–‡ä»¶åè§£æ
            if not original_url:
                # ç®€å•çš„æ–‡ä»¶åè§£æé€»è¾‘
                original_url = filename.split('_', 1)[-1].rsplit('.', 1)[0]

            # 4. å¹¶è¡Œç”Ÿæˆæ‘˜è¦å’Œæ ‡ç­¾
            summary, tags = await self.generate_summary_tags_async(md_content)
            logger.info(f"Summary: {summary[:100]}...")
            logger.info(f"Tags: {', '.join(tags)}")
            
            # 5. å‘é€ Telegram é€šçŸ¥
            notification = (
                f"âœ¨ æ–°çš„ç½‘é¡µå‰ªè—\n\n"
                f"ğŸ“‘ {title}\n\n"
                f"ğŸ“ {summary}\n\n"
                f"ğŸ”— åŸå§‹é“¾æ¥ï¼š{original_url}\n"
                f"ğŸ“š å¿«ç…§é“¾æ¥ï¼š{github_url}"
            )
            await self.send_telegram_notification(notification)
            
            logger.info("=" * 50)
            logger.info("Web page handle finished...")
            logger.info("=" * 50)
            
            return {
                "status": "success",
                "github_url": github_url,
                "title": title,
                "summary": summary,
                "tags": tags,
                "original_url": original_url
            }
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            logger.error("=" * 50)
            await self.send_telegram_notification(error_msg)
            raise
        finally:
            # ç¡®ä¿æ¸…ç†èµ„æº
            pass

    def upload_to_github(self, html_path):
        """çº¿ç¨‹å®‰å…¨çš„ GitHub ä¸Šä¼ """
        # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„ GitHub å®¢æˆ·ç«¯
        github_client = Github(self.github_token)
        
        try:
            filename = os.path.basename(html_path)
            
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            repo = github_client.get_repo(self.github_repo_name)
            file_path = f"clips/{filename}"
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            try:
                existing_file = repo.get_contents(file_path, ref="main")
                # å¦‚æœå­˜åœ¨ï¼Œæ›´æ–°æ–‡ä»¶
                repo.update_file(
                    file_path,
                    f"Update web clip: {filename}",
                    content,
                    existing_file.sha,
                    branch="main"
                )
            except Exception:
                # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
                repo.create_file(
                    file_path,
                    f"Add web clip: {filename}",
                    content,
                    branch="main"
                )
            
            github_url = f"https://{self.config['github_pages_domain']}/{self.github_repo_name.split('/')[1]}/clips/{filename}"
            
            # ç­‰å¾…éƒ¨ç½²ï¼ˆåœ¨çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
            max_retries = self.config.get('github_pages_max_retries', 60)
            for attempt in range(max_retries):
                try:
                    response = requests.get(github_url, timeout=10)
                    if response.status_code == 200:
                        break
                    time.sleep(5)
                except Exception:
                    time.sleep(5)
            
            return filename, github_url
            
        finally:
            # æ¸…ç† GitHub å®¢æˆ·ç«¯
            if hasattr(github_client, 'close'):
                github_client.close()

    async def url2md_async(self, url, max_retries=30):
        """å¼‚æ­¥ URL è½¬ Markdown"""
        md_url = f"https://r.jina.ai/{url}"
        
        for attempt in range(max_retries):
            try:
                async with self.session.get(md_url, timeout=30) as response:
                    if response.status == 200:
                        md_content = await response.text()
                        return md_content
                    else:
                        await asyncio.sleep(10)
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                await asyncio.sleep(10)
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        return await run_in_threadpool(self.get_page_content_by_bs, url)

    async def generate_summary_tags_async(self, content):
        """å¼‚æ­¥ç”Ÿæˆæ‘˜è¦å’Œæ ‡ç­¾"""
        for attempt in range(self.config.get('max_retries', 3)):
            try:
                response = await self.openai_client.chat.completions.create(
                    model=self.config.get('openai_model', 'gpt-3.5-turbo'),
                    messages=[{
                        "role": "user",
                        "content": """è¯·ä¸ºä»¥ä¸‹ç½‘é¡µ(å·²è½¬æ¢ä¸ºMarkdownæ ¼å¼)å†…å®¹ç”Ÿæˆç®€çŸ­æ‘˜è¦å’Œç›¸å…³æ ‡ç­¾ã€‚ 
                        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›(è‹±æ–‡ç½‘é¡µè¯·ä»¥ä¸­æ–‡è¿”å›)ï¼š
                        æ‘˜è¦ï¼š[100å­—ä»¥å†…çš„æ‘˜è¦]
                        æ ‡ç­¾ï¼štag1ï¼Œtag2ï¼Œtag3ï¼Œtag4ï¼Œtag5

                        ç½‘é¡µ(å·²è½¬æ¢ä¸ºmarkdownæ ¼å¼)å†…å®¹ï¼š
                        """ + content[:5000] + "..."
                    }],
                    timeout=60
                )

                result = response.choices[0].message.content
                
                try:
                    lines = result.split('\n')
                    summary_line = next((line for line in lines if line.startswith('æ‘˜è¦ï¼š')), None)
                    tags_line = next((line for line in lines if line.startswith('æ ‡ç­¾ï¼š')), None)
                    
                    if summary_line and tags_line:
                        summary = summary_line.replace('æ‘˜è¦ï¼š', '').strip()
                        tags_str = tags_line.replace('æ ‡ç­¾ï¼š', '').strip()
                        tags = [
                            tag.strip()[:20]
                            for tag in tags_str.replace('ï¼Œ', ',').split(',')
                            if tag.strip()
                        ]
                        return summary, tags
                    
                except Exception as e:
                    logger.error(f"è§£æ AI å“åº”å¤±è´¥: {str(e)}")
                
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
                return "è‡ªåŠ¨ç”Ÿæˆçš„æ‘˜è¦", ["ç½‘é¡µå‰ªè—"]
                
            except Exception as e:
                logger.warning(f"OpenAI API è°ƒç”¨å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
                if attempt < self.config.get('max_retries', 3) - 1:
                    await asyncio.sleep(self.config.get('retry_delay', 2))
        
        return "æ— æ³•ç”Ÿæˆæ‘˜è¦", ["æœªåˆ†ç±»"]

    def get_page_content_by_md(self, md_content):
        """ä» markdown è·å–æ ‡é¢˜"""
        lines = md_content.splitlines()
        for line in lines:
            if line.startswith("Title:"):
                return line.replace("Title:", "").strip()
            if line.startswith("# "):
                return line.replace("# ", "").strip()
        return "æœªçŸ¥æ ‡é¢˜"

    def get_page_content_by_bs(self, url):
        """ä»éƒ¨ç½²çš„é¡µé¢è·å–å†…å®¹"""
        try:
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è·å–æ ‡é¢˜
                title = None
                if soup.title:
                    title = soup.title.string
                if not title and soup.h1:
                    title = soup.h1.get_text(strip=True)
                
                # æå–æ­£æ–‡å†…å®¹
                html2markdown = html2text.HTML2Text()
                html2markdown.ignore_links = True
                html2markdown.ignore_images = True
                content = html2markdown.handle(soup.prettify())
                
                return f"Title: {title if title else 'æœªçŸ¥æ ‡é¢˜'} \n\n {content}"
                
        except Exception as e:
            logger.error(f"BeautifulSoup å¤„ç†å¤±è´¥: {str(e)}")
        
        return "Title: æœªçŸ¥æ ‡é¢˜ \n\n å†…å®¹è·å–å¤±è´¥"

    async def send_telegram_notification(self, message):
        """å‘é€ Telegram é€šçŸ¥"""
        try:
            await self.telegram_bot.send_message(
                chat_id=self.config['telegram_chat_id'],
                text=message,
                parse_mode='HTML'
            )
        except Exception as e:
            logger.error(f"Telegram é€šçŸ¥å‘é€å¤±è´¥: {str(e)}")

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global handler
    handler = WebClipperHandler(CONFIG)
    UPLOAD_DIR.mkdir(exist_ok=True)
    
    # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ API keyï¼Œç”Ÿæˆä¸€ä¸ª
    if 'api_key' not in CONFIG:
        CONFIG['api_key'] = secrets.token_urlsafe(32)
        logger.info(f"Generated new API key: {CONFIG['api_key']}")

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    if handler:
        await handler.close_session()
    executor.shutdown(wait=False)

@app.post("/upload")
async def upload_file(
    request: Request,
    token: str = Depends(verify_token)
):
    """æ”¯æŒå¹¶å‘å¤„ç†çš„ä¸Šä¼ æ¥å£"""
    try:
        # æ£€æŸ¥å¹¶å‘é™åˆ¶
        current_requests = getattr(request.state, 'active_requests', 0)
        if current_requests >= CONFIG.get('max_concurrent_requests', 50):
            raise HTTPException(status_code=429, detail="Too many concurrent requests")
        
        form = await request.form()
        original_url = form.get('url', '')
        
        # è·å–æ–‡ä»¶
        file = None
        for field_name, field_value in form.items():
            if hasattr(field_value, 'filename') and hasattr(field_value, 'read'):
                file = field_value
                break
        
        if not file:
            raise HTTPException(status_code=400, detail="No file found in form data")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        filename = file.filename
        
        # éªŒè¯æ–‡ä»¶
        file_ext = Path(filename).suffix.lower()
        if not file_ext:
            filename += '.html'
            file_ext = '.html'
        
        if file_ext not in ALLOWED_EXTENSIONS:
            raise HTTPException(
                status_code=400,
                detail=f"File type not allowed. Allowed types: {', '.join(ALLOWED_EXTENSIONS)}"
            )
        
        if len(content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail=f"File too large. Maximum size allowed: {MAX_FILE_SIZE/1024/1024}MB"
            )
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        safe_filename = f"{int(time.time())}_{secrets.token_hex(8)}_{filename}"
        file_path = UPLOAD_DIR / safe_filename
        
        # ä½¿ç”¨çº¿ç¨‹æ± å†™å…¥æ–‡ä»¶
        def write_file():
            with open(file_path, "wb") as f:
                f.write(content)
        
        await run_in_threadpool(write_file)
        
        try:
            # å¼‚æ­¥å¤„ç†æ–‡ä»¶
            result = await handler.process_file(file_path, original_url)
            return result
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if file_path.exists():
                def remove_file():
                    try:
                        file_path.unlink()
                    except:
                        pass
                
                await run_in_threadpool(remove_file)
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload failed: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "concurrent_workers": executor._max_workers
    }

@app.get("/stats")
async def get_stats():
    """è·å–æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "max_workers": executor._max_workers,
        "active_threads": executor._work_queue.qsize(),
        "upload_dir_size": sum(f.stat().st_size for f in UPLOAD_DIR.glob('*') if f.is_file()),
        "upload_dir_files": len(list(UPLOAD_DIR.glob('*')))
    }

def start_server(host="0.0.0.0", port=8000):
    """å¯åŠ¨æ”¯æŒå¹¶å‘çš„æœåŠ¡å™¨"""
    uvicorn.run(
        app, 
        host=host, 
        port=port,
        loop="asyncio",
        timeout_keep_alive=30,
        limit_concurrency=100,
        limit_max_requests=1000,
        log_level="info"
    )

